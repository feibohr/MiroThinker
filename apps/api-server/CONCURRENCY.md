# ğŸš€ å¹¶å‘æ€§èƒ½ä¼˜åŒ–æŒ‡å—

## é—®é¢˜è¯Šæ–­

### Gradio Demo çš„å¹¶å‘ç“¶é¢ˆ

1. **å…¨å±€å…±äº« Pipeline ç»„ä»¶** âš ï¸
   ```python
   # æ‰€æœ‰è¯·æ±‚å…±äº«åŒä¸€ä¸ª tool_manager
   _preload_cache["main_agent_tool_manager"]  # å…¨å±€å•ä¾‹
   ```
   **å½±å“**: å·¥å…·è°ƒç”¨å¯èƒ½ä¸²è¡ŒåŒ–ï¼Œå¯¼è‡´è¯·æ±‚æ’é˜Ÿ

2. **LLM API é™åˆ¶** âš ï¸
   - è‡ªéƒ¨ç½²æ¨¡å‹æœåŠ¡å™¨é€šå¸¸æœ‰å¹¶å‘é™åˆ¶
   - GPU èµ„æºç«äº‰
   - å¦‚æœæœåŠ¡å™¨å•çº¿ç¨‹å¤„ç†ï¼Œåˆ™å®Œå…¨ä¸²è¡Œ

3. **E2B Sandbox åˆ›å»º** âš ï¸
   - ä»£ç æ‰§è¡Œæ²™ç®±åˆ›å»ºéœ€è¦æ—¶é—´ï¼ˆ2-5ç§’ï¼‰
   - API æœ‰é€Ÿç‡é™åˆ¶
   - å¹¶å‘åˆ›å»ºå¯èƒ½å—é™

## API Server çš„å¹¶å‘ä¼˜åŒ–

### æ ¸å¿ƒæ”¹è¿›

#### 1. Pipeline è¿æ¥æ± 

```python
# services/concurrency_manager.py
class PipelinePool:
    """
    ç»´æŠ¤å¤šä¸ªç‹¬ç«‹çš„ Pipeline å®ä¾‹
    æ¯ä¸ªå®ä¾‹æœ‰ç‹¬ç«‹çš„ tool_managerï¼Œé¿å…èµ„æºç«äº‰
    """
    def __init__(self, cfg, pool_size: int = 5):
        # åˆ›å»º 5 ä¸ªç‹¬ç«‹çš„ Pipeline å®ä¾‹
        for i in range(pool_size):
            self.pool.append({
                "main_agent_tool_manager": ...,  # ç‹¬ç«‹å®ä¾‹
                "sub_agent_tool_managers": ...,
                "output_formatter": ...,
            })
```

**ä¼˜åŠ¿:**
- âœ… æ¯ä¸ªè¯·æ±‚è·å–ç‹¬ç«‹çš„ Pipeline å®ä¾‹
- âœ… é¿å…å…±äº«çŠ¶æ€å¯¼è‡´çš„ç«äº‰
- âœ… æ”¯æŒçœŸæ­£çš„å¹¶å‘æ‰§è¡Œ

#### 2. å¹¶å‘é™æµ

```python
class ConcurrencyLimiter:
    """
    å…¨å±€å¹¶å‘é™åˆ¶ï¼Œé˜²æ­¢ç³»ç»Ÿè¿‡è½½
    """
    def __init__(self, max_concurrent: int = 10):
        self._semaphore = asyncio.Semaphore(max_concurrent)
```

**ä¼˜åŠ¿:**
- âœ… é™åˆ¶åŒæ—¶å¤„ç†çš„è¯·æ±‚æ•°
- âœ… è¶…å‡ºé™åˆ¶çš„è¯·æ±‚æ’é˜Ÿç­‰å¾…
- âœ… é˜²æ­¢èµ„æºè€—å°½

#### 3. è¯·æ±‚ç”Ÿå‘½å‘¨æœŸç®¡ç†

```python
async def stream_research(self, task_id: str, query: str):
    # è·å– Pipeline å®ä¾‹
    instance = await self.pipeline_manager.acquire_pipeline()
    
    try:
        # ä½¿ç”¨ç‹¬ç«‹çš„å®ä¾‹æ‰§è¡Œä»»åŠ¡
        await execute_task_pipeline(
            main_agent_tool_manager=instance["main_agent_tool_manager"],
            ...
        )
    finally:
        # é‡Šæ”¾å›è¿æ¥æ± 
        self.pipeline_manager.release_pipeline(instance)
```

## é…ç½®è°ƒä¼˜

### ç¯å¢ƒå˜é‡

```bash
# .env

# Pipeline è¿æ¥æ± å¤§å°ï¼ˆæ¯ä¸ªå®ä¾‹ç‹¬ç«‹å¤„ç†ä¸€ä¸ªè¯·æ±‚ï¼‰
PIPELINE_POOL_SIZE=5

# æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼ˆè¶…å‡ºåˆ™æ’é˜Ÿï¼‰
MAX_CONCURRENT_REQUESTS=10
```

### æ¨èé…ç½®

| åœºæ™¯ | PIPELINE_POOL_SIZE | MAX_CONCURRENT_REQUESTS | è¯´æ˜ |
|------|-------------------|------------------------|------|
| **è½»é‡ä½¿ç”¨** | 3 | 5 | ä¸ªäººå¼€å‘ã€æµ‹è¯• |
| **ä¸­ç­‰æµé‡** | 5 | 10 | å°å›¢é˜Ÿä½¿ç”¨ |
| **é«˜æµé‡** | 10 | 20 | ç”Ÿäº§ç¯å¢ƒ |
| **èµ„æºå—é™** | 2 | 3 | ä½é…æœåŠ¡å™¨ |

### è°ƒä¼˜å»ºè®®

1. **PIPELINE_POOL_SIZE**:
   - æ¯ä¸ªå®ä¾‹éœ€è¦ä¸€å®šå†…å­˜ï¼ˆ~500MB-1GBï¼‰
   - ä¸è¦è¶…è¿‡ CPU æ ¸å¿ƒæ•°çš„ 2 å€
   - è€ƒè™‘ E2B sandbox åˆ›å»ºé™åˆ¶

2. **MAX_CONCURRENT_REQUESTS**:
   - åº”è¯¥ >= PIPELINE_POOL_SIZE
   - è€ƒè™‘ LLM API çš„å¹¶å‘é™åˆ¶
   - è€ƒè™‘ Serper/Jina API çš„é€Ÿç‡é™åˆ¶

## æ€§èƒ½æµ‹è¯•

### è¿è¡Œå¹¶å‘æµ‹è¯•

```bash
cd /Users/feibohr/Documents/workspace/git/python/MiroThinker/apps/api-server

# å®‰è£…æµ‹è¯•ä¾èµ–
uv pip install httpx

# è¿è¡Œå¹¶å‘åŸºå‡†æµ‹è¯•
uv run python benchmark_concurrency.py
```

### é¢„æœŸç»“æœ

**ä¼˜åŒ–å‰ï¼ˆGradio Demoï¼‰:**
```
Sequential (3 requests): 120s total
Concurrent (3 requests): 110s total
Speedup: 1.09x âŒ (å‡ ä¹æ²¡æœ‰åŠ é€Ÿ)
```

**ä¼˜åŒ–åï¼ˆAPI Serverï¼‰:**
```
Sequential (3 requests): 120s total
Concurrent (3 requests): 45s total
Speedup: 2.67x âœ… (æ¥è¿‘çº¿æ€§åŠ é€Ÿ)
```

## ç“¶é¢ˆåˆ†æ

å³ä½¿ä¼˜åŒ–åï¼Œä»å¯èƒ½é‡åˆ°ä»¥ä¸‹ç“¶é¢ˆï¼š

### 1. LLM API é™åˆ¶ ğŸ”´

**é—®é¢˜:**
```bash
# ä½ çš„ LLM æœåŠ¡å™¨
BASE_URL=http://192.168.56.66:8114/v1

# å¯èƒ½çš„é™åˆ¶ï¼š
- å•çº¿ç¨‹æ¨ç†
- GPU èµ„æºç«äº‰
- vLLM/SGLang çš„å¹¶å‘é…ç½®
```

**è§£å†³:**
```bash
# vLLM å¯åŠ¨æ—¶å¢åŠ å¹¶å‘
python -m vllm.entrypoints.openai.api_server \
    --model mirothinker \
    --max-num-seqs 10  # å¢åŠ å¹¶å‘åºåˆ—æ•°
    
# SGLang å¯åŠ¨æ—¶å¢åŠ å¹¶å‘
python -m sglang.launch_server \
    --model-path mirothinker \
    --dp 2  # æ•°æ®å¹¶è¡Œï¼Œå¢åŠ åå
```

### 2. å·¥å…· API é™æµ ğŸ”´

**Serper API:**
- å…è´¹ç‰ˆ: 100 requests/day
- ä»˜è´¹ç‰ˆ: 1000-5000 requests/hour

**Jina API:**
- é€Ÿç‡é™åˆ¶å¯èƒ½å¯¼è‡´ 429 é”™è¯¯

**E2B:**
- Sandbox åˆ›å»ºæœ‰å¹¶å‘é™åˆ¶
- æ¯ä¸ªè´¦æˆ·æœ‰æœ€å¤§ sandbox æ•°é‡

**è§£å†³:**
- å‡çº§ API å¥—é¤
- ä½¿ç”¨å¤šä¸ª API key è½®æ¢
- å®ç°è¯·æ±‚ç¼“å­˜

### 3. ç½‘ç»œå»¶è¿Ÿ ğŸ”´

**é—®é¢˜:**
- å·¥å…· API è°ƒç”¨ä¸²è¡Œ
- æ¯æ¬¡æœç´¢ + çˆ¬å– = 5-10 ç§’

**è§£å†³:**
```python
# å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼ˆéœ€è¦æ¡†æ¶æ”¯æŒï¼‰
results = await asyncio.gather(
    search_api.call(),
    scrape_api.call(),
    code_api.call(),
)
```

## ç›‘æ§å’Œè¯Šæ–­

### 1. å¥åº·æ£€æŸ¥

```bash
curl http://localhost:8000/health
```

è¿”å›ï¼š
```json
{
  "status": "healthy",
  "active_requests": 3,
  "pool_size": 5
}
```

### 2. å®æ—¶ç›‘æ§

```python
# æ·»åŠ  Prometheus metrics
from prometheus_client import Counter, Gauge

request_counter = Counter('mirothinker_requests_total', 'Total requests')
active_requests = Gauge('mirothinker_active_requests', 'Active requests')
pipeline_pool_usage = Gauge('mirothinker_pool_usage', 'Pipeline pool usage')
```

### 3. æ—¥å¿—åˆ†æ

```bash
# æŸ¥çœ‹å¹¶å‘æ—¥å¿—
grep "Concurrency:" logs/api-server.log

# è¾“å‡ºç¤ºä¾‹ï¼š
# Concurrency: 3/10 active requests
# Acquired pipeline instance 2 for task xxx
# Released pipeline instance 2 for task xxx
```

## æœ€ä½³å®è·µ

### 1. é€æ­¥å¢åŠ å¹¶å‘

```bash
# ä»å°å¼€å§‹
PIPELINE_POOL_SIZE=2
MAX_CONCURRENT_REQUESTS=3

# ç›‘æ§æ€§èƒ½å’Œé”™è¯¯ç‡
# å¦‚æœç¨³å®šï¼Œé€æ­¥å¢åŠ 
```

### 2. ä½¿ç”¨è´Ÿè½½å‡è¡¡

```bash
# docker-compose.yml
services:
  api-server-1:
    ...
  api-server-2:
    ...
  
  nginx:
    # è´Ÿè½½å‡è¡¡åˆ°å¤šä¸ªå®ä¾‹
```

### 3. å®ç°è¯·æ±‚ç¼“å­˜

```python
# ç¼“å­˜æœç´¢ç»“æœ
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_search(query: str):
    return serper_api.search(query)
```

## æ•…éšœæ’æŸ¥

### é—®é¢˜ï¼šè¯·æ±‚ä»ç„¶å¾ˆæ…¢

**æ£€æŸ¥:**
1. æŸ¥çœ‹ active_requests æ˜¯å¦è¾¾åˆ° pool_size
2. æ£€æŸ¥ LLM API å“åº”æ—¶é—´
3. æŸ¥çœ‹å·¥å…· API æ˜¯å¦è¿”å› 429 é”™è¯¯
4. ç›‘æ§ CPU/å†…å­˜ä½¿ç”¨

### é—®é¢˜ï¼šå†…å­˜ä¸è¶³

**è§£å†³:**
```bash
# å‡å°‘ pool size
PIPELINE_POOL_SIZE=2

# æˆ–å¢åŠ æœåŠ¡å™¨å†…å­˜
```

### é—®é¢˜ï¼šå·¥å…·è°ƒç”¨è¶…æ—¶

**è§£å†³:**
```python
# å¢åŠ è¶…æ—¶æ—¶é—´
@with_timeout(1800)  # 30åˆ†é’Ÿ
async def execute_tool_call(...):
    ...
```

## æ€»ç»“

| ä¼˜åŒ–é¡¹ | Gradio Demo | API Server | æ”¹è¿› |
|--------|-------------|-----------|------|
| Pipeline å¤ç”¨ | å…¨å±€å•ä¾‹ âš ï¸ | è¿æ¥æ±  âœ… | é¿å…ç«äº‰ |
| å¹¶å‘é™åˆ¶ | æ—  âš ï¸ | Semaphore âœ… | é˜²æ­¢è¿‡è½½ |
| è¯·æ±‚æ’é˜Ÿ | éšæœº âš ï¸ | æœ‰åºé˜Ÿåˆ— âœ… | å…¬å¹³è°ƒåº¦ |
| ç›‘æ§æŒ‡æ ‡ | æ—  âš ï¸ | å¥åº·æ£€æŸ¥ âœ… | å¯è§‚æµ‹æ€§ |
| ç†è®ºåŠ é€Ÿæ¯” | 1.0-1.2x | 2.0-3.0x | 2-3å€æå‡ |

**æ ¸å¿ƒå»ºè®®:**
- âœ… ä½¿ç”¨ API Server è€Œé Gradio Demo ç”¨äºç”Ÿäº§ç¯å¢ƒ
- âœ… æ ¹æ®ç¡¬ä»¶èµ„æºè°ƒæ•´ pool_size
- âœ… ç›‘æ§ LLM API çš„å¹¶å‘é™åˆ¶
- âœ… è€ƒè™‘å‡çº§å·¥å…· API å¥—é¤ä»¥æé«˜é™æµé˜ˆå€¼

